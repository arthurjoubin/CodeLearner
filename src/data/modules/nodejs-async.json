{
  "module": {
    "id": "nodejs-async",
    "title": "Node.js: Asynchronous Programming",
    "description": "Master asynchronicity in Node.js: callbacks, Promises, async/await, EventEmitter, and Streams",
    "icon": "Zap",
    "requiredXp": 200,
    "color": "from-yellow-500 to-orange-500",
    "courseId": "node-express"
  },
  "lessons": [
    {
      "id": "async-callbacks",
      "moduleId": "nodejs-async",
      "title": "Callbacks and Callback Hell",
      "order": 1,
      "difficulty": "beginner",
      "content": "# Essential to know\n- Callbacks are the foundation of Node.js asynchronicity\n- Convention: first parameter = error (error-first callbacks)\n- \"Callback hell\" makes code hard to read\n- Promises and async/await have replaced callbacks\n\n---\n\n# History: Callbacks in Node.js\n\nNode.js was created with an entirely callback-based approach to handle asynchronicity. It's important to understand this because much legacy code still uses this syntax.\n\n## Callback Syntax\n\nA callback is a function passed as an argument that will be executed later:\n\n```javascript\n// Old fs API with callbacks\nimport { readFile } from 'fs';\n\nreadFile('./data.txt', 'utf-8', (err, data) => {\n  if (err) {\n    console.error('Error:', err);\n    return;\n  }\n  console.log('Content:', data);\n});\n```\n\n## The Error-First Convention\n\nAll Node.js callbacks follow the same convention:\n- **First parameter**: error (or null if no error)\n- **Following parameters**: data\n\n```javascript\nfunction callback(error, result) {\n  if (error) {\n    // Handle error\n    return;\n  }\n  // Use the result\n}\n```\n\n## The Callback Hell (Pyramid of Doom)\n\nWhen you chain multiple asynchronous operations, the code becomes unreadable:\n\n```javascript\n// ðŸ˜± CALLBACK HELL - AVOID\nreadFile('config.json', 'utf8', (err, config) => {\n  if (err) { console.error(err); return; }\n  \n  parseConfig(config, (err, parsed) => {\n    if (err) { console.error(err); return; }\n    \n    connectDB(parsed.dbUrl, (err, db) => {\n      if (err) { console.error(err); return; }\n      \n      db.query('SELECT * FROM users', (err, users) => {\n        if (err) { console.error(err); return; }\n        \n        writeFile('output.json', JSON.stringify(users), (err) => {\n          if (err) { console.error(err); return; }\n          console.log('Done!');\n        });\n      });\n    });\n  });\n});\n```\n\n## Solutions to Callback Hell\n\n### 1. Named Functions\n```javascript\n// Better: extract callbacks\nreadFile('config.json', 'utf8', handleConfig);\n\nfunction handleConfig(err, config) {\n  if (err) return console.error(err);\n  parseConfig(config, handleParsed);\n}\n\nfunction handleParsed(err, parsed) {\n  if (err) return console.error(err);\n  connectDB(parsed.dbUrl, handleDB);\n}\n\nfunction handleDB(err, db) {\n  if (err) return console.error(err);\n  db.query('SELECT * FROM users', handleUsers);\n}\n\nfunction handleUsers(err, users) {\n  if (err) return console.error(err);\n  writeFile('output.json', JSON.stringify(users), handleWrite);\n}\n\nfunction handleWrite(err) {\n  if (err) return console.error(err);\n  console.log('Done!');\n}\n```\n\n### 2. Promises (Modern)\n```javascript\n// âœ… Use modern fs/promises API\nimport { readFile, writeFile } from 'fs/promises';\n\nasync function processData() {\n  const config = await readFile('config.json', 'utf8');\n  const parsed = await parseConfig(config);\n  const db = await connectDB(parsed.dbUrl);\n  const users = await db.query('SELECT * FROM users');\n  await writeFile('output.json', JSON.stringify(users));\n  console.log('Done!');\n}\n```\n\n## Legacy APIs to Know\n\nSome APIs still use callbacks:\n```javascript\n// setTimeout / setInterval\nsetTimeout(() => {\n  console.log('After 1 second');\n}, 1000);\n\n// Event handlers\nprocess.on('exit', (code) => {\n  console.log(`Process exited: ${code}`);\n});\n\n// Some database drivers\ndb.query('SELECT *', (err, results) => {\n  // ...\n});\n```\n\n## Converting Callbacks to Promises\n\n```javascript\nimport { promisify } from 'util';\nimport { readFile } from 'fs';\n\n// Convert callback API to Promise\nconst readFileAsync = promisify(readFile);\n\n// Now use with async/await\nasync function loadData() {\n  const data = await readFileAsync('./file.txt', 'utf8');\n  return data;\n}\n```"
    },
    {
      "id": "async-promises",
      "moduleId": "nodejs-async",
      "title": "Promises: then/catch/finally",
      "order": 2,
      "difficulty": "intermediate",
      "content": "# Essential to know\n- Promise represents a future value (available later)\n- `.then()` for success, `.catch()` for errors\n- `.finally()` executes in all cases\n- Chaining possible to sequence operations\n\n---\n\n# Understanding Promises\n\nA Promise is an object that represents an asynchronous operation that:\n- **Pending**: In progress\n- **Fulfilled**: Completed successfully (resolved)\n- **Rejected**: Completed with error\n\n## Creating a Promise\n\n```javascript\nconst myPromise = new Promise((resolve, reject) => {\n  // Asynchronous operation\n  setTimeout(() => {\n    const success = true;\n    \n    if (success) {\n      resolve('Data loaded!');  // Success\n    } else {\n      reject(new Error('Loading failed'));  // Error\n    }\n  }, 1000);\n});\n```\n\n## Consuming a Promise\n\n### With then/catch\n```javascript\nfetchUserData(userId)\n  .then(user => {\n    console.log('User:', user);\n    return fetchUserPosts(user.id);\n  })\n  .then(posts => {\n    console.log('Posts:', posts);\n  })\n  .catch(error => {\n    console.error('Error:', error.message);\n  })\n  .finally(() => {\n    console.log('Operation completed');\n  });\n```\n\n### With async/await (recommended)\n```javascript\nasync function loadUserData(userId) {\n  try {\n    const user = await fetchUserData(userId);\n    console.log('User:', user);\n    \n    const posts = await fetchUserPosts(user.id);\n    console.log('Posts:', posts);\n  } catch (error) {\n    console.error('Error:', error.message);\n  } finally {\n    console.log('Operation completed');\n  }\n}\n```\n\n## Chaining Promises\n\n```javascript\nfunction fetchData(url) {\n  return fetch(url)\n    .then(response => {\n      if (!response.ok) {\n        throw new Error(`HTTP ${response.status}`);\n      }\n      return response.json();\n    });\n}\n\n// Chaining\nfetchData('/api/user')\n  .then(user => fetchData(`/api/posts?user=${user.id}`))\n  .then(posts => fetchData(`/api/comments?post=${posts[0].id}`))\n  .then(comments => console.log(comments))\n  .catch(err => console.error(err));\n```\n\n## Promise.all - Parallel Execution\n\n```javascript\n// Execute in parallel\nconst [users, posts, comments] = await Promise.all([\n  fetchUsers(),\n  fetchPosts(),\n  fetchComments()\n]);\n\n// With error handling per promise\nconst results = await Promise.all([\n  fetchUsers().catch(() => []),  // Empty array if error\n  fetchPosts().catch(() => []),\n  fetchComments().catch(() => [])\n]);\n```\n\n## Promise.race - First to Complete\n\n```javascript\n// First promise to complete wins\nconst result = await Promise.race([\n  fetchFromPrimaryAPI(),\n  fetchFromBackupAPI()\n]);\n\n// Timeout pattern\nconst timeout = new Promise((_, reject) =>\n  setTimeout(() => reject(new Error('Timeout')), 5000)\n);\n\nconst data = await Promise.race([fetchData(), timeout]);\n```\n\n## Promise.allSettled - All Results\n\n```javascript\n// Get all results, whether success or failure\nconst results = await Promise.allSettled([\n  fetchUser(1),\n  fetchUser(2),\n  fetchUser(999)  // Might fail\n]);\n\n// Results:\n// [\n//   { status: 'fulfilled', value: {...} },\n//   { status: 'fulfilled', value: {...} },\n//   { status: 'rejected', reason: Error(...) }\n// ]\n\n// Process results\nconst successful = results\n  .filter(r => r.status === 'fulfilled')\n  .map(r => r.value);\n```\n\n## Error Handling Patterns\n\n```javascript\n// Pattern 1: try/catch with async/await\nasync function getData() {\n  try {\n    const result = await fetchData();\n    return result;\n  } catch (err) {\n    console.error('Failed:', err);\n    return null;  // Fallback\n  }\n}\n\n// Pattern 2: Catch and rethrow\nasync function getData() {\n  try {\n    return await fetchData();\n  } catch (err) {\n    // Add context\n    throw new Error(`Failed to fetch data: ${err.message}`);\n  }\n}\n\n// Pattern 3: Multiple catches\nfetchData()\n  .catch(err => {\n    // Try fallback\n    return fetchBackupData();\n  })\n  .catch(err => {\n    // Final fallback\n    return defaultData;\n  })\n  .then(data => console.log(data));\n```"
    },
    {
      "id": "async-eventemitter",
      "moduleId": "nodejs-async",
      "title": "EventEmitter: Event-Driven Programming",
      "order": 3,
      "difficulty": "intermediate",
      "content": "# Essential to know\n- EventEmitter is at the heart of Node.js (event-based)\n- `.on()` listens to an event, `.emit()` triggers it\n- Observer pattern to decouple components\n- Used by Streams, HTTP servers, process, etc.\n\n---\n\n# The Event-Driven Architecture of Node.js\n\nNode.js is built around the EventEmitter pattern. Many native objects inherit from it:\n- HTTP Server\n- Streams (fs, HTTP, process)\n- Process (events 'exit', 'uncaughtException')\n- Network sockets\n\n## Creating an EventEmitter\n\n```javascript\nimport { EventEmitter } from 'events';\n\nconst myEmitter = new EventEmitter();\n\n// Listen to an event\nmyEmitter.on('greet', (name) => {\n  console.log(`Hello, ${name}!`);\n});\n\n// Trigger the event\nmyEmitter.emit('greet', 'Alice');  // Hello, Alice!\nmyEmitter.emit('greet', 'Bob');    // Hello, Bob!\n```\n\n## Essential Methods\n\n```javascript\n// .on() - Listen to every occurrence\nemitter.on('event', listener);\n\n// .once() - Listen only once\nemitter.once('init', () => console.log('Initialized!'));\n\n// .emit() - Trigger the event\nemitter.emit('event', arg1, arg2);\n\n// .off() or .removeListener() - Remove a listener\nemitter.off('event', listener);\n\n// .removeAllListeners() - Remove all listeners\nemitter.removeAllListeners('event');\n```\n\n## Practical Pattern: Class with Events\n\n```javascript\nimport { EventEmitter } from 'events';\n\nclass DataProcessor extends EventEmitter {\n  constructor() {\n    super();\n    this.queue = [];\n  }\n  \n  async process(item) {\n    this.emit('start', item);\n    \n    try {\n      const result = await this.transform(item);\n      this.emit('success', result);\n      return result;\n    } catch (error) {\n      this.emit('error', error);\n      throw error;\n    }\n  }\n  \n  async transform(item) {\n    // Processing...\n    return { ...item, processed: true };\n  }\n}\n\n// Usage\nconst processor = new DataProcessor();\n\nprocessor.on('start', (item) => {\n  console.log(`Processing: ${item.id}`);\n});\n\nprocessor.on('success', (result) => {\n  console.log(`Completed: ${result.id}`);\n});\n\nprocessor.on('error', (err) => {\n  console.error('Processing failed:', err);\n});\n\nawait processor.process({ id: 1, data: 'test' });\n```\n\n## Built-in EventEmitters\n\n### HTTP Server\n```javascript\nimport { createServer } from 'http';\n\nconst server = createServer();\n\nserver.on('request', (req, res) => {\n  console.log(`${req.method} ${req.url}`);\n});\n\nserver.on('connection', (socket) => {\n  console.log('New connection');\n});\n\nserver.on('error', (err) => {\n  console.error('Server error:', err);\n});\n\nserver.listen(3000);\n```\n\n### Process Events\n```javascript\n// Before exit\nprocess.on('beforeExit', (code) => {\n  console.log('Process about to exit:', code);\n});\n\n// Uncaught exception\nprocess.on('uncaughtException', (err) => {\n  console.error('Uncaught exception:', err);\n  process.exit(1);\n});\n\n// Signals\nprocess.on('SIGINT', () => {\n  console.log('Ctrl+C pressed, shutting down...');\n  process.exit(0);\n});\n\nprocess.on('SIGTERM', () => {\n  console.log('SIGTERM received, shutting down...');\n  process.exit(0);\n});\n```\n\n### Stream Events\n```javascript\nimport { createReadStream } from 'fs';\n\nconst stream = createReadStream('file.txt');\n\nstream.on('data', (chunk) => {\n  console.log('Chunk:', chunk.length);\n});\n\nstream.on('end', () => {\n  console.log('Finished reading');\n});\n\nstream.on('error', (err) => {\n  console.error('Stream error:', err);\n});\n```\n\n## Memory Leak Warning\n\n```javascript\nconst emitter = new EventEmitter();\n\n// By default, warning after 10 listeners\nemitter.setMaxListeners(20);  // Increase limit\n\n// Always remove listeners when done\nconst handler = () => console.log('event');\nemitter.on('event', handler);\n// ... later\nemitter.off('event', handler);  // Clean up\n```\n\n## Event Names Best Practices\n\n```javascript\n// Use constants for event names\nconst Events = {\n  USER_CREATED: 'user:created',\n  USER_UPDATED: 'user:updated',\n  ORDER_PLACED: 'order:placed'\n};\n\n// Namespace with colons\nemitter.on(Events.USER_CREATED, (user) => {\n  // Send welcome email\n});\n\nemitter.on(Events.ORDER_PLACED, (order) => {\n  // Update inventory\n});\n```"
    },
    {
      "id": "async-streams",
      "moduleId": "nodejs-async",
      "title": "Streams: Data Flow Processing",
      "order": 4,
      "difficulty": "advanced",
      "content": "# Essential to know\n- Streams process data in chunks\n- Avoid loading entire files into memory\n- 4 types: Readable, Writable, Duplex, Transform\n- Pipe() to chain streams\n\n---\n\n# Why Use Streams?\n\nImagine processing a 2GB file:\n- **Without streams**: Load 2GB into RAM â†’ Likely crash\n- **With streams**: Process 64KB at a time â†’ Stable memory\n\n## Types of Streams\n\n1. **Readable**: Reading (fs.createReadStream, HTTP request)\n2. **Writable**: Writing (fs.createWriteStream, HTTP response)\n3. **Duplex**: Both (TCP sockets)\n4. **Transform**: Duplex + transformation (compression, encryption)\n\n## Readable Streams\n\n```javascript\nimport { createReadStream } from 'fs';\n\nconst stream = createReadStream('./huge-file.txt', {\n  encoding: 'utf-8',\n  highWaterMark: 1024  // Chunk size (1KB)\n});\n\n// Listen for data\nstream.on('data', (chunk) => {\n  console.log('Chunk received:', chunk.length, 'characters');\n});\n\nstream.on('end', () => {\n  console.log('Reading completed');\n});\n\nstream.on('error', (err) => {\n  console.error('Error:', err);\n});\n```\n\n## Writable Streams\n\n```javascript\nimport { createWriteStream } from 'fs';\n\nconst stream = createWriteStream('./output.txt');\n\nstream.write('First line\n');\nstream.write('Second line\n');\nstream.end('Last line\n');  // End the stream\n\nstream.on('finish', () => {\n  console.log('Writing completed');\n});\n```\n\n## Piping: Chaining Streams\n\n```javascript\nimport { createReadStream, createWriteStream } from 'fs';\nimport { createGzip } from 'zlib';\n\n// Copy a file\ncreateReadStream('./input.txt')\n  .pipe(createWriteStream('./output.txt'));\n\n// Compress a file\ncreateReadStream('./input.txt')\n  .pipe(createGzip())\n  .pipe(createWriteStream('./input.txt.gz'));\n```\n\n## With async/await (pipeline)\n\n```javascript\nimport { pipeline } from 'stream/promises';\nimport { createReadStream, createWriteStream } from 'fs';\nimport { createGzip } from 'zlib';\n\nasync function compressFile() {\n  try {\n    await pipeline(\n      createReadStream('./input.txt'),\n      createGzip(),\n      createWriteStream('./output.txt.gz')\n    );\n    console.log('Compression completed');\n  } catch (err) {\n    console.error('Error:', err);\n  }\n}\n```\n\n## Transform Stream\n\n```javascript\nimport { Transform } from 'stream';\n\nconst upperCaseTransform = new Transform({\n  transform(chunk, encoding, callback) {\n    // chunk = Buffer, convert to uppercase\n    this.push(chunk.toString().toUpperCase());\n    callback();\n  }\n});\n\n// Usage\nprocess.stdin\n  .pipe(upperCaseTransform)\n  .pipe(process.stdout);\n\n// Type \"hello\" â†’ Displays \"HELLO\"\n```\n\n## Practical Example: CSV Processing\n\n```javascript\nimport { createReadStream } from 'fs';\nimport { createInterface } from 'readline';\n\nasync function processCSV(filename) {\n  const fileStream = createReadStream(filename);\n  const rl = createInterface({\n    input: fileStream,\n    crlfDelay: Infinity\n  });\n\n  let lineCount = 0;\n  for await (const line of rl) {\n    const [name, email, age] = line.split(',');\n    console.log(`User: ${name}, ${email}, ${age}`);\n    lineCount++;\n  }\n  \n  console.log(`Processed ${lineCount} lines`);\n}\n```\n\n## HTTP with Streams\n\n```javascript\nimport { createServer } from 'http';\nimport { createReadStream } from 'fs';\nimport { createGzip } from 'zlib';\n\nconst server = createServer((req, res) => {\n  // Stream a large file\n  const stream = createReadStream('./large-file.zip');\n  \n  // Set headers\n  res.setHeader('Content-Type', 'application/zip');\n  res.setHeader('Content-Disposition', 'attachment; filename=\"file.zip\"');\n  \n  // Pipe to response (which is a Writable stream)\n  stream.pipe(res);\n  \n  // Handle errors\n  stream.on('error', (err) => {\n    res.statusCode = 500;\n    res.end('Error reading file');\n  });\n});\n\nserver.listen(3000);\n```\n\n## Backpressure Management\n\n```javascript\nconst readable = createReadStream('large.txt');\nconst writable = createWriteStream('output.txt');\n\nreadable.on('data', (chunk) => {\n  // Check if writable is ready\n  if (!writable.write(chunk)) {\n    readable.pause();  // Pause reading\n    \n    writable.once('drain', () => {\n      readable.resume();  // Resume reading\n    });\n  }\n});\n\nreadable.on('end', () => {\n  writable.end();\n});\n```\n\n## When to Use Streams\n\n| Use Case | Without Streams | With Streams |\n|----------|----------------|--------------|\n| 2GB file | âŒ Crash | âœ… Stable |\n| Video upload | âŒ Wait for complete | âœ… Start processing |\n| API response | âŒ JSON.stringify | âœ… Streaming JSON |\n| Real-time | âŒ Batch processing | âœ… Continuous |"
    },
    {
      "id": "async-env",
      "moduleId": "nodejs-async",
      "title": "Environment Variables",
      "order": 5,
      "difficulty": "intermediate",
      "content": "# Essential to know\n- `process.env` contains environment variables\n- Use the `dotenv` package to load .env\n- Never put secrets in code (use .env)\n- Different configs per environment (dev/staging/prod)\n\n---\n\n# Managing Configuration with Environment Variables\n\nEnvironment variables allow configuring your application without modifying code. Essential for:\n- **Secrets**: API keys, database passwords\n- **Configuration**: Server port, API URLs\n- **Feature flags**: Enable/disable features\n- **Environments**: Dev vs Production\n\n## Reading Environment Variables\n\n```javascript\n// process.env is an object containing all variables\nconst port = process.env.PORT || 3000;\nconst dbUrl = process.env.DATABASE_URL;\nconst nodeEnv = process.env.NODE_ENV || 'development';\n\nconsole.log(`Server started on port ${port}`);\n```\n\n## The .env File\n\nCreate a `.env` file at the root:\n\n```bash\n# .env\nNODE_ENV=development\nPORT=3000\nDATABASE_URL=postgresql://user:password@localhost:5432/mydb\nJWT_SECRET=your-super-secret-jwt-never-commit-this\nAPI_KEY=sk-1234567890\n```\n\n## Load .env File with dotenv\n\n```bash\nnpm install dotenv\n```\n\n```javascript\n// At the very beginning of your main file (index.js/app.js)\nimport 'dotenv/config';\n\n// Or with CommonJS\nrequire('dotenv').config();\n\n// Now process.env contains values from .env\nconsole.log(process.env.DATABASE_URL);\n```\n\n## Structured Configuration\n\nCreate a `config.js` file to centralize:\n\n```javascript\n// config.js\nimport 'dotenv/config';\n\nconst config = {\n  env: process.env.NODE_ENV || 'development',\n  port: parseInt(process.env.PORT, 10) || 3000,\n  \n  database: {\n    url: process.env.DATABASE_URL,\n    poolSize: parseInt(process.env.DB_POOL_SIZE, 10) || 10\n  },\n  \n  jwt: {\n    secret: process.env.JWT_SECRET,\n    expiresIn: process.env.JWT_EXPIRES_IN || '7d'\n  },\n  \n  api: {\n    key: process.env.API_KEY,\n    url: process.env.API_URL || 'https://api.example.com'\n  }\n};\n\n// Validation\nif (!config.database.url) {\n  throw new Error('DATABASE_URL is required');\n}\n\nif (!config.jwt.secret) {\n  throw new Error('JWT_SECRET is required');\n}\n\nexport default config;\n```\n\n## Usage in Application\n\n```javascript\n// app.js\nimport config from './config.js';\n\nconst app = express();\n\n// Use configuration\napp.listen(config.port, () => {\n  console.log(`Server running on port ${config.port}`);\n});\n\n// Database connection\nconst db = await connectDB(config.database.url);\n```\n\n## Environment-Specific Files\n\n```bash\n# .env.development\nNODE_ENV=development\nPORT=3000\nDATABASE_URL=postgresql://localhost:5432/devdb\nDEBUG=true\n\n# .env.production\nNODE_ENV=production\nPORT=8080\nDATABASE_URL=postgresql://prod-db:5432/proddb\nDEBUG=false\n```\n\n```javascript\n// Load specific file\nimport dotenv from 'dotenv';\n\ndotenv.config({\n  path: `.env.${process.env.NODE_ENV || 'development'}`\n});\n```\n\n## Best Practices\n\n### 1. Never Commit .env\n```bash\n# .gitignore\n.env\n.env.local\n.env.*.local\n```\n\n### 2. Provide Example\n```bash\n# .env.example (commit this)\nNODE_ENV=development\nPORT=3000\nDATABASE_URL=postgresql://user:password@localhost:5432/dbname\nJWT_SECRET=change-me-in-production\n```\n\n### 3. Validate Required Variables\n```javascript\nconst required = ['DATABASE_URL', 'JWT_SECRET'];\n\nfor (const key of required) {\n  if (!process.env[key]) {\n  throw new Error(`Missing required environment variable: ${key}`);\n  }\n}\n```\n\n### 4. Type Conversion\n```javascript\n// Booleans\nconst debug = process.env.DEBUG === 'true';\n\n// Numbers\nconst port = parseInt(process.env.PORT, 10) || 3000;\nconst timeout = parseInt(process.env.TIMEOUT_MS, 10) || 5000;\n\n// Arrays\nconst corsOrigins = process.env.CORS_ORIGINS?.split(',') || [];\n```\n\n## Common Patterns\n\n### Feature Flags\n```javascript\nconst features = {\n  newDashboard: process.env.FEATURE_NEW_DASHBOARD === 'true',\n  betaApi: process.env.FEATURE_BETA_API === 'true'\n};\n\nif (features.newDashboard) {\n  app.use('/dashboard/v2', newDashboardRouter);\n}\n```\n\n### Conditional Logging\n```javascript\nconst isDev = process.env.NODE_ENV === 'development';\n\nif (isDev) {\n  app.use(morgan('dev'));\n  app.use(errorHandler({ showStack: true }));\n} else {\n  app.use(morgan('combined'));\n}\n```\n\n### Multiple Environments\n```javascript\nconst environments = {\n  development: {\n    logLevel: 'debug',\n    enableMocks: true\n  },\n  staging: {\n    logLevel: 'info',\n    enableMocks: false\n  },\n  production: {\n    logLevel: 'warn',\n    enableMocks: false\n  }\n};\n\nconst env = environments[process.env.NODE_ENV] || environments.development;\n```"
    }
  ],
  "exercises": []
}
