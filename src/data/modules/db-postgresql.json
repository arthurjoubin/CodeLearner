{
  "module": {
    "id": "db-postgresql",
    "title": "PostgreSQL: Professional Database",
    "description": "Master PostgreSQL for production applications: installation, optimization and advanced features",
    "icon": "Database",
    "requiredXp": 900,
    "color": "from-indigo-600 to-purple-600",
    "courseId": "databases"
  },
  "lessons": [
    {
      "id": "postgres-intro",
      "moduleId": "db-postgresql",
      "title": "Introduction to PostgreSQL",
      "order": 1,
      "difficulty": "intermediate",
      "content": "# Essential to know\\n- PostgreSQL = advanced open-source relational database\\n- Modern features: JSONB, arrays, full-text search\\n- Strict ACID, concurrent (MVCC)\\n- Extensible via extensions (PostGIS, TimescaleDB)\\n- Industry standard for serious applications\\n\\n---\\n\\n# Why PostgreSQL?\\n\\nPostgreSQL (often called \"Postgres\") is considered the most advanced open-source relational database. Created in 1986, it combines reliability, advanced features, and SQL standards compliance.\\n\\n## PostgreSQL strengths\\n\\n### 1. SQL Compliance\\n- Strict SQL standards compliance\\n- Portable to other systems\\n- Consistent and documented syntax\\n\\n### 2. Rich data types\\n- Standard types: INTEGER, TEXT, BOOLEAN, DATE/TIMESTAMP\\n- Arrays: INTEGER[], TEXT[]\\n- JSONB: Indexable binary JSON\\n- Geospatial with PostGIS\\n- Custom types\\n\\n### 3. Reliability\\n- Strict ACID\\n- MVCC (Multi-Version Concurrency Control): non-blocking reads\\n- Write-Ahead Logging (WAL): no data corruption\\n- Point-in-time recovery\\n\\n### 4. Extensibility\\n- PostGIS: Geospatial data\\n- TimescaleDB: Time-series data\\n- pg_trgm: Text similarity\\n- uuid-ossp: UUID generation\\n- And hundreds more\\n\\n## Local installation\\n\\n### macOS (with Homebrew)\\n```bash\\nbrew install postgresql@15\\nbrew services start postgresql@15\\n\\n# Create database for your user\\ncreatedb $(whoami)\\n```\\n\\n### Linux (Ubuntu/Debian)\\n```bash\\nsudo apt update\\nsudo apt install postgresql postgresql-contrib\\nsudo systemctl start postgresql\\n\\n# Change postgres password\\nsudo -u postgres psql -c \\\"ALTER USER postgres WITH PASSWORD 'your_password';\\\"\\n```\\n\\n### Docker (recommended for dev)\\n```bash\\ndocker run --name postgres-dev \\\\\\\\n  -e POSTGRES_PASSWORD=password \\\\\\\\n  -e POSTGRES_DB=myapp \\\\\\\\n  -p 5432:5432 \\\\\\\\n  -v postgres_data:/var/lib/postgresql/data \\\\\\\\n  -d postgres:15\\n```..."
    },
    {
      "id": "postgres-node",
      "moduleId": "db-postgresql",
      "title": "PostgreSQL with Node.js",
      "order": 2,
      "difficulty": "intermediate",
      "content": "# Essential to know\\n- Use `pg` (node-postgres) the standard driver\\n- Connection pool for performance\\n- Parameterized queries against SQL injection\\n- Transactions with BEGIN/COMMIT/ROLLBACK\\n- Migrations with node-pg-migrate or other\\n\\n---\\n\\n# pg (node-postgres)\\n\\nThe `pg` package is the standard PostgreSQL driver for Node.js. It supports SSL connections, notifications, and more.\\n\\n## Installation and setup\\n\\n```bash\\nnpm install pg\\n```\\n\\n```javascript\\nimport { Pool, Client } from 'pg';\\n\\n// Pool (recommended for web apps)\\nconst pool = new Pool({\\n  connectionString: process.env.DATABASE_URL,\\n  ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false,\\n  max: 20,                    // Max concurrent connections\\n  idleTimeoutMillis: 30000,   // Close idle connections after 30s\\n  connectionTimeoutMillis: 2000,\\n});\\n\\n// Test connection\\npool.on('connect', () => {\\n  console.log('Connected to PostgreSQL');\\n});\\n\\npool.on('error', (err) => {\\n  console.error('PostgreSQL error', err);\\n});\\n```\\n\\n## Basic queries\\n\\n```javascript\\n// Simple query\\nconst result = await pool.query('SELECT NOW()');\\nconsole.log(result.rows[0].now);\\n\\n// With parameters (SECURITY: prevents SQL injection)\\nconst userId = 1;\\nconst result = await pool.query(\\n  'SELECT * FROM users WHERE id = $1',\\n  [userId]\\n);\\n\\n// Multiple parameters\\nawait pool.query(\\n  'INSERT INTO users (name, email, age) VALUES ($1, $2, $3)',\\n  ['John', 'john@example.com', 30]\\n);\\n\\n// Get with LIMIT/OFFSET\\nconst { rows } = await pool.query(\\n  'SELECT * FROM users LIMIT $1 OFFSET $2',\\n  [20, 0]  // LIMIT 20 OFFSET 0\\n);\\n```\\n\\n## Repository Pattern\\n\\n```javascript\\nclass UserRepository {\\n  constructor(pool) {\\n    this.pool = pool;\\n  }\\n  \\n  async findById(id) {\\n    const { rows } = await this.pool.query(\\n      'SELECT * FROM users WHERE id = $1',\\n      [id]\\n    );\\n    return rows[0] || null;\\n..."
    },
    {
      "id": "postgres-advanced",
      "moduleId": "db-postgresql",
      "title": "Advanced features",
      "order": 3,
      "difficulty": "advanced",
      "content": "# Essential to know\\n- Indexes to speed up queries (B-tree, Hash, GIN, GIST)\\n- Built-in full-text search\\n- Materialized views for aggregated data\\n- Partitions for large tables\\n- Backup with pg_dump and point-in-time recovery\\n\\n---\\n\\n# Indexing\\n\\nIndexes speed up queries but slow down writes. Use them judiciously.\\n\\n## Index types\\n\\n```sql\\n-- B-tree (default): equality and range\\nCREATE INDEX idx_users_email ON users(email);\\nCREATE INDEX idx_orders_date ON orders(created_at);\\n\\n-- Composite\\nCREATE INDEX idx_orders_user_date ON orders(user_id, created_at);\\n\\n-- Unique\\nCREATE UNIQUE INDEX idx_users_lower_email ON users(LOWER(email));\\n\\n-- Partial (with condition)\\nCREATE INDEX idx_active_users ON users(email) WHERE active = true;\\n\\n-- GIN for JSONB and arrays\\nCREATE INDEX idx_events_data ON events USING GIN(data);\\n\\n-- GiST for geospatial data\\nCREATE INDEX idx_locations_coords ON locations USING GIST(coords);\\n```\\n\\n## Full-Text Search\\n\\n```sql\\n-- Enable extension\\nCREATE EXTENSION IF NOT EXISTS pg_trgm;\\n\\n-- tsvector column for search\\nALTER TABLE articles ADD COLUMN search_vector tsvector;\\n\\n-- Update with data\\nUPDATE articles SET\\n  search_vector = setweight(to_tsvector('english', title), 'A') ||\\n                  setweight(to_tsvector('english', content), 'B');\\n\\n-- GIN index\\nCREATE INDEX idx_articles_search ON articles USING GIN(search_vector);\\n\\n-- Search\\nSELECT * FROM articles\\nWHERE search_vector @@ to_tsquery('english', 'postgresql & tutorial');\\n\\n-- Rank by relevance\\nSELECT\\n  title,\\n  ts_rank(search_vector, query) AS rank\\nFROM articles, to_tsquery('english', 'postgresql') query\\nWHERE search_vector @@ query\\nORDER BY rank DESC;\\n```\\n\\n## Views and Materialized Views\\n\\n```sql\\n-- View (stored query)\\nCREATE VIEW active_users AS\\nSELECT id, name, email FROM users WHERE active = true;\\n\\nSELECT * FROM active_users;..."
    }
  ],
  "exercises": [
    {
      "id": "postgres-ex-1",
      "lessonId": "postgres-node",
      "moduleId": "db-postgresql",
      "title": "Fix the Connection Pool Setup",
      "difficulty": "easy",
      "type": "code",
      "description": "Copilot generated this PostgreSQL connection code, but your app keeps crashing with 'Client has already been released'. Fix it!",
      "instructions": "Your AI assistant generated this code to connect to PostgreSQL:\n\n```javascript\nimport { Client } from 'pg';\n\nconst client = new Client({\n  connectionString: process.env.DATABASE_URL\n});\n\nawait client.connect();\n\nexport async function getUser(id) {\n  const result = await client.query('SELECT * FROM users WHERE id = $1', [id]);\n  return result.rows[0];\n}\n```\n\nThis works fine for the first request, but subsequent requests fail with errors about the client being closed or released.\n\n**Fix the code** to use the proper connection pattern for a web application.\n\n**Expected behavior:** The database connection should be reusable across multiple requests without crashing.\n\n**Hint:** Think about the difference between a single Client and a Pool!",
      "starterCode": "import { Client } from 'pg';\n\nconst client = new Client({\n  connectionString: process.env.DATABASE_URL\n});\n\nawait client.connect();\n\nexport async function getUser(id) {\n  const result = await client.query('SELECT * FROM users WHERE id = $1', [id]);\n  return result.rows[0];\n}",
      "solution": "import { Pool } from 'pg';\n\nconst pool = new Pool({\n  connectionString: process.env.DATABASE_URL\n});\n\nexport async function getUser(id) {\n  const result = await pool.query('SELECT * FROM users WHERE id = $1', [id]);\n  return result.rows[0];\n}",
      "hints": [
        "Client is for single connections - it connects once and disconnects. Not good for web apps!",
        "Pool manages multiple reusable connections - perfect for web applications",
        "Change 'Client' to 'Pool' and remove the await client.connect() call",
        "Pool automatically manages connections, so you don't need to manually connect/disconnect",
        "The pool will handle connection reuse, error recovery, and cleanup automatically"
      ],
      "validationPrompt": "Check if the student's code:\n1. Imports and uses Pool instead of Client\n2. Removes the manual await client.connect() call (Pool handles this automatically)\n3. Uses pool.query() instead of client.query()\n4. Still uses parameterized queries with $1 placeholder\n5. The connection pattern is suitable for a web application (reusable pool)\n\nThe key fix is using Pool for connection pooling instead of a single Client. Accept variations in variable naming (pool, db, etc.) as long as they use Pool."
    },
    {
      "id": "postgres-ex-2",
      "lessonId": "postgres-node",
      "moduleId": "db-postgresql",
      "title": "Use PostgreSQL-specific RETURNING",
      "difficulty": "intermediate",
      "type": "code",
      "description": "ChatGPT wrote an INSERT query but you need to get the created user's ID back. Use PostgreSQL's RETURNING clause!",
      "instructions": "Your AI tool generated this code to create a user:\n\n```javascript\nawait pool.query(\n  'INSERT INTO users (name, email) VALUES ($1, $2)',\n  ['Alice', 'alice@example.com']\n);\n\n// Now we need to query again to get the ID\nconst result = await pool.query(\n  'SELECT id FROM users WHERE email = $1',\n  ['alice@example.com']\n);\nconst userId = result.rows[0].id;\n```\n\nThis works but requires TWO database queries. PostgreSQL has a better way!\n\n**Rewrite this code** to use PostgreSQL's `RETURNING` clause to get the inserted user's ID in a single query.\n\n**Expected behavior:** Insert the user and get their ID back in one query.\n\n**Bonus:** Return the entire user object (id, name, email, created_at) using RETURNING *",
      "starterCode": "await pool.query(\n  'INSERT INTO users (name, email) VALUES ($1, $2)',\n  ['Alice', 'alice@example.com']\n);\n\nconst result = await pool.query(\n  'SELECT id FROM users WHERE email = $1',\n  ['alice@example.com']\n);\nconst userId = result.rows[0].id;",
      "solution": "const result = await pool.query(\n  'INSERT INTO users (name, email) VALUES ($1, $2) RETURNING *',\n  ['Alice', 'alice@example.com']\n);\nconst user = result.rows[0];\nconst userId = user.id;",
      "hints": [
        "PostgreSQL supports RETURNING clause at the end of INSERT, UPDATE, and DELETE",
        "Add RETURNING * to get all columns of the inserted row",
        "Or use RETURNING id to get just the ID",
        "This is a PostgreSQL-specific feature (MySQL doesn't have it)",
        "You can combine the two queries into one: INSERT ... VALUES ... RETURNING *"
      ],
      "validationPrompt": "Check if the student's code:\n1. Uses a single query (not two separate queries)\n2. Has RETURNING clause in the INSERT statement\n3. Either uses RETURNING * or RETURNING id (both are acceptable)\n4. Accesses result.rows[0] to get the inserted data\n5. The INSERT still uses parameterized queries ($1, $2)\n\nThe key improvement is using PostgreSQL's RETURNING clause to get data back from INSERT in one query instead of two. Accept variations like RETURNING id, created_at or RETURNING * - all are valid."
    },
    {
      "id": "postgres-ex-3",
      "lessonId": "postgres-node",
      "moduleId": "db-postgresql",
      "title": "Debug Pool Exhaustion Error",
      "difficulty": "intermediate",
      "type": "code",
      "description": "Your API is throwing 'TimeoutError: timeout acquiring client from pool'. AI suggested increasing max connections, but that's not the root cause!",
      "instructions": "Your Express API keeps crashing with this error:\n\n```\nTimeoutError: timeout acquiring client from pool after 2000ms\n```\n\nChatGPT suggested increasing the pool size, but you found this suspicious code:\n\n```javascript\nexport async function updateUser(id, data) {\n  const client = await pool.connect();\n  \n  try {\n    const result = await client.query(\n      'UPDATE users SET name = $1, email = $2 WHERE id = $3 RETURNING *',\n      [data.name, data.email, id]\n    );\n    return result.rows[0];\n  } catch (error) {\n    console.error('Update failed:', error);\n    return null;\n  }\n}\n```\n\n**Find and fix the bug** that's causing the pool to run out of connections.\n\n**Expected behavior:** The pool should properly release connections even when errors occur.\n\n**Hint:** What happens to the client when there's an error? What happens when there's no error?",
      "starterCode": "export async function updateUser(id, data) {\n  const client = await pool.connect();\n  \n  try {\n    const result = await client.query(\n      'UPDATE users SET name = $1, email = $2 WHERE id = $3 RETURNING *',\n      [data.name, data.email, id]\n    );\n    return result.rows[0];\n  } catch (error) {\n    console.error('Update failed:', error);\n    return null;\n  }\n}",
      "solution": "export async function updateUser(id, data) {\n  const client = await pool.connect();\n  \n  try {\n    const result = await client.query(\n      'UPDATE users SET name = $1, email = $2 WHERE id = $3 RETURNING *',\n      [data.name, data.email, id]\n    );\n    return result.rows[0];\n  } catch (error) {\n    console.error('Update failed:', error);\n    return null;\n  } finally {\n    client.release();\n  }\n}",
      "hints": [
        "When you call pool.connect(), you MUST call client.release() to return it to the pool",
        "Without release(), the connection stays checked out forever and the pool runs out",
        "The current code never releases the client - not on success, not on error!",
        "Use a finally block to guarantee the client is released no matter what",
        "Or better: use pool.query() directly instead of pool.connect() for simple queries"
      ],
      "validationPrompt": "Check if the student's code:\n1. Adds a finally block with client.release() call\n2. OR rewrites to use pool.query() directly (even better solution)\n3. The client is released in ALL code paths (success and error)\n4. The try/catch/finally structure is correct\n\nThe bug is that client.release() is never called, causing pool exhaustion. Accept either solution:\n- Adding finally { client.release() }\n- Removing pool.connect() and using pool.query() directly (better for simple queries)\n\nBoth are correct fixes. The key is ensuring connections are always returned to the pool."
    },
    {
      "id": "postgres-ex-4",
      "lessonId": "postgres-intro",
      "moduleId": "db-postgresql",
      "title": "Understanding PostgreSQL Data Types",
      "difficulty": "intermediate",
      "type": "quiz",
      "question": "You asked Cursor to create a 'products' table. It generated this:\n\n```sql\nCREATE TABLE products (\n  id INTEGER PRIMARY KEY,\n  name VARCHAR(255),\n  price DECIMAL(10, 2),\n  tags TEXT,\n  metadata TEXT\n);\n```\n\nYou want to store multiple tags per product and JSON metadata. What's the most PostgreSQL-native way to do this?",
      "options": [
        "Change tags to TEXT[] (array) and metadata to JSONB for better querying and indexing",
        "Keep tags as TEXT with comma-separated values and metadata as TEXT with JSON.parse()",
        "Use tags VARCHAR(255) and metadata VARCHAR(1000) to limit size",
        "Create separate tables: product_tags and product_metadata for normalization"
      ],
      "correctAnswer": 0,
      "explanation": "PostgreSQL has native support for arrays and JSON that you should use instead of storing them as text:\n\n**Best approach (PostgreSQL-native):**\n```sql\nCREATE TABLE products (\n  id SERIAL PRIMARY KEY,           -- Auto-incrementing\n  name TEXT,                       -- No length limit needed\n  price DECIMAL(10, 2),\n  tags TEXT[],                     -- Native array type\n  metadata JSONB                   -- Binary JSON (indexable)\n);\n```\n\n**Why this is better:**\n\n1. **SERIAL instead of INTEGER**: Auto-incrementing primary key\n2. **TEXT[] array**: Query with ANY, ALL, array operators\n   ```sql\n   SELECT * FROM products WHERE 'electronics' = ANY(tags);\n   ```\n3. **JSONB instead of TEXT**: Indexable, queryable, validates JSON\n   ```sql\n   SELECT * FROM products WHERE metadata->>'color' = 'red';\n   CREATE INDEX ON products USING GIN(metadata);\n   ```\n4. **TEXT instead of VARCHAR**: In PostgreSQL, TEXT has no performance penalty\n\nAI tools often generate generic SQL that works everywhere, but PostgreSQL-specific types give you better performance and easier querying!"
    }
  ]
}